{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch pandas scikit-learn mlflow matplotlib seaborn wordcloud tensorflow tensorflow-data-validation zenml tweepy cassandra-driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_data_validation as tfdv\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zenml.steps import step\n",
    "from zenml.pipelines import pipeline\n",
    "import tweepy\n",
    "import json\n",
    "import requests\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "\n",
    "# Constants and Hyperparameters\n",
    "CSV_FILE_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv'  # Path to the dataset\n",
    "PRETRAINED_LM_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden'  # Path to the pre-trained language model\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 16,          # Batch size for training\n",
    "    \"learning_rate\": 2e-5,     # Learning rate for the optimizer\n",
    "    \"epochs\": 2,               # Number of training epochs\n",
    "    \"weight_decay\": 0.01,      # Weight decay for regularization\n",
    "    'max_grad_norm': 1.0,      # Maximum gradient norm for gradient clipping\n",
    "    'lr_step_size': 1,         # Step size for learning rate scheduler\n",
    "    'lr_gamma': 0.1,           # Gamma factor for learning rate scheduler\n",
    "}\n",
    "\n",
    "# Print hyperparameters\n",
    "print(\"Hyperparameters:\", HYPERPARAMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the CSV file path is correct and accessible\n",
    "if not os.path.exists(CSV_FILE_PATH):\n",
    "    raise FileNotFoundError(f\"CSV file not found at: {CSV_FILE_PATH}\")\n",
    "\n",
    "# Load data from CSV file into a DataFrame\n",
    "data = pd.read_csv(CSV_FILE_PATH)  # Reading the dataset from CSV file\n",
    "\n",
    "# Additional Features and Comments:\n",
    "# - This step loads the dataset from a CSV file into a pandas DataFrame for further processing.\n",
    "# - It assumes that the CSV file is structured with rows representing data instances and columns representing features.\n",
    "# - Ensure that the CSV file path is correct and accessible.\n",
    "# - It's advisable to check the structure of the loaded data (e.g., using data.head()) to ensure it's loaded correctly.\n",
    "# - Further data preprocessing steps such as handling missing values, encoding categorical variables, or feature scaling\n",
    "#   can be applied depending on the specific requirements of the machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tweet_id as the index of the DataFrame\n",
    "data.set_index('tweet_id', inplace=True)  # Setting 'tweet_id' as the index inplace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the DataFrame 'data' to quickly inspect the initial entries\n",
    "# and check for the presence and format of data in the columns.\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are any null values in the entire DataFrame 'data'.\n",
    "if data.isnull().values.any():\n",
    "    # Calculate the sum of null values for each column and store it in 'null_counts'.\n",
    "    null_counts = data.isnull().sum()\n",
    "    \n",
    "    # Print a warning message to indicate the presence of null values in the dataset.\n",
    "    print(\"Warning: Null values found in the dataset.\")\n",
    "    \n",
    "    # Print the count of null values for each column where the count is greater than zero.\n",
    "    print(f\"Null value counts by column:\\n{null_counts[null_counts > 0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of expected column names for the dataset, specifically 'text' and 'label'.\n",
    "# This set will be used to verify that the dataset contains these essential columns.\n",
    "expected_columns = {'text', 'label'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a concise summary of the DataFrame 'data', including the number of entries,\n",
    "# the name and type of each column, the number of non-null values in each column,\n",
    "# and the memory usage. This helps in getting an overview of the dataset's structure\n",
    "# and any potential issues with data types or missing values.\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    <class 'pandas.core.frame.DataFrame'> \n",
    "    # This indicates that 'data' is a pandas DataFrame, a 2D size-mutable, potentially heterogeneous tabular data structure with labeled axes.\n",
    "\n",
    "    Int64Index: 11 entries, 1298098337946783745 to 1239394168939503617 \n",
    "    # The DataFrame 'data' uses an Integer index (Int64Index) and contains 11 entries.\n",
    "    # These entries are indexed from 1298098337946783745 to 1239394168939503617.\n",
    "\n",
    "    Data columns (total 2 columns): \n",
    "    # The DataFrame consists of two columns.\n",
    "\n",
    "    #   Column  Non-Null Count  Dtype \n",
    "    ---  ------  --------------  ----- \n",
    "    0   text    11 non-null     object \n",
    "    # Column 'text' has 11 entries, all of which are non-null (i.e., no missing values).\n",
    "    # The datatype (Dtype) of this column is 'object', typically used in pandas to denote string data.\n",
    "\n",
    "    1   label   11 non-null     object\n",
    "    # Column 'label' also has 11 non-null entries, indicating no missing values in this column either.\n",
    "    # This column's datatype is also 'object', suggesting that it may contain string data.\n",
    "\n",
    "    dtypes: object(2)\n",
    "    # This confirms that both columns in the DataFrame are of type 'object'.\n",
    "\n",
    "    memory usage: 264.0 bytes\n",
    "    # The entire DataFrame consumes 264 bytes of memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and return a Series containing the count of missing values in each column of the DataFrame 'data'.\n",
    "# This function checks for null values across each column and sums them up, providing a quick overview\n",
    "# of the data cleanliness and where missing values are concentrated.\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access and return the 11th entry (index 10) from the 'text' column of the DataFrame 'data'.\n",
    "# This is done using the 'iloc' indexer, which works on the basis of integer index positions.\n",
    "# This specific call retrieves the data from the 11th position in the 'text' column, useful for inspecting or processing a specific item.\n",
    "data.text.iloc[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and return the counts of unique values in the 'label' column of the DataFrame 'data'.\n",
    "# This method tallies how many times each unique value appears in the 'label' column,\n",
    "# providing a quick summary of the distribution of categories within the data.\n",
    "# It is useful for understanding the balance or imbalance among different classes in a dataset.\n",
    "data.label.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt  # Import matplotlib's pyplot module for creating static, interactive, and animated visualizations in Python.\n",
    "import seaborn as sns           # Import seaborn, a Python data visualization library based on matplotlib, providing a high-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "# Set up the plot dimensions\n",
    "plt.figure(figsize=(40, 5))  # Create a new figure with a specified figure size of 40 inches in width and 5 inches in height.\n",
    "\n",
    "# Create a count plot\n",
    "sns.countplot(data.label, palette='Spectral')  # Use seaborn's countplot to show the counts of observations in each categorical bin using bars.\n",
    "                                               # 'data.label' specifies the column in DataFrame from which to draw the categories.\n",
    "                                               # 'palette' is set to 'Spectral' to define the color palette of the plot.\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Classes')  # Set the x-axis label of the plot to 'Classes'.\n",
    "plt.title('Class Distribution')  # Set the title of the plot to 'Class Distribution'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a pie chart\n",
    "plt.figure(figsize=(8, 8))  # Set up the figure size for the pie chart\n",
    "class_counts = data.label.value_counts()  # Count occurrences of each class in the 'label' column\n",
    "plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=90, colors=plt.cm.Spectral(np.linspace(0, 1, len(class_counts))))  # Create a pie chart\n",
    "plt.title('Proportion of Each Class')  # Add a title to the pie chart\n",
    "\n",
    "# Display the plot\n",
    "plt.show()  # Show the pie chart in the output cell of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def data_analysis_step(CSV_FILE_PATH: str) -> Output(null_counts=pd.Series, class_distribution_fig=object):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(CSV_FILE_PATH)\n",
    "    data.set_index('tweet_id', inplace=True)\n",
    "\n",
    "    # Display head of the dataframe\n",
    "    print(data.head())\n",
    "\n",
    "    # Check for null values\n",
    "    if data.isnull().values.any():\n",
    "        null_counts = data.isnull().sum()\n",
    "        print(\"Warning: Null values found in the dataset.\")\n",
    "        print(f\"Null value counts by column:\\n{null_counts[null_counts > 0]}\")\n",
    "\n",
    "    # Display dataset info\n",
    "    data.info()\n",
    "\n",
    "    # Count nulls\n",
    "    print(data.isnull().sum())\n",
    "\n",
    "    # Access a specific element\n",
    "    print(data.text.iloc[10])\n",
    "\n",
    "    # Value counts for 'label'\n",
    "    print(data.label.value_counts())\n",
    "\n",
    "    # Plotting the class distribution\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.countplot(x=data.label, palette='Spectral')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    # Return the null_counts and the figure (the figure as an example, handling of plt.figure in real applications may vary)\n",
    "    return null_counts, plt.gcf()\n",
    "\n",
    "# Note: In real deployment, you might want to handle outputs differently, such as saving them to files or databases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "        \"\"\"\n",
    "        Function to clean text data.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        # Removing URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Removing usernames and hashtags\n",
    "        text = re.sub(r'@\\S+|#\\S+', '', text)\n",
    "        # Removing special characters and numbers\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        # Converting to lowercase\n",
    "        text = text.lower().strip()\n",
    "        # Tokenize and rejoin the text to ensure clean tokenization\n",
    "        tokens = text.split()\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of expected column names for the dataset: 'text' and 'label'.\n",
    "expected_columns = {'text', 'label'}\n",
    "\n",
    "# Check if the expected columns are present in the DataFrame 'data'.\n",
    "if not expected_columns.issubset(data.columns):\n",
    "    # Calculate the set difference to identify which expected columns are missing from the DataFrame.\n",
    "    missing_cols = expected_columns - set(data.columns)\n",
    "    \n",
    "    # Construct an error message stating which columns are missing.\n",
    "    error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "    \n",
    "    # Log the error message using the logging library.\n",
    "    logging.error(error_msg)\n",
    "    \n",
    "    # Raise a ValueError to stop execution and alert the user to the missing necessary columns.\n",
    "    raise ValueError(error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of expected column names and their expected data types for the dataset: 'text' as string and 'label' as string.\n",
    "expected_columns_and_types = {'text': 'object', 'label': 'object'}\n",
    "\n",
    "# Check if the expected columns are present in the DataFrame 'data'.\n",
    "if not set(expected_columns_and_types.keys()).issubset(data.columns):\n",
    "    missing_cols = set(expected_columns_and_types.keys()) - set(data.columns)\n",
    "    error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "    logging.error(error_msg)\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# Verify that each column has the correct data type.\n",
    "incorrect_type_cols = {col: expected_type for col, expected_type in expected_columns_and_types.items()\n",
    "                       if data[col].dtype != expected_type}\n",
    "if incorrect_type_cols:\n",
    "    error_msg_type = f\"The dataframe has incorrect data types for the columns: {', '.join(f'{col}: expected {etype}, found {data[col].dtype}' for col, etype in incorrect_type_cols.items())}\"\n",
    "    logging.error(error_msg_type)\n",
    "    raise ValueError(error_msg_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a message to indicate that data preprocessing is beginning.\n",
    "print(\"Preprocessing the data...\")\n",
    "\n",
    "# Identify and store rows where the 'label' column is missing data.\n",
    "missing_label_rows = data[data['label'].isna()]\n",
    "\n",
    "# Check if there are any rows with missing labels and handle them accordingly.\n",
    "if not missing_label_rows.empty:\n",
    "    print(f\"Found {len(missing_label_rows)} rows with missing labels.\")\n",
    "    # Option to handle missing data: Remove, Impute, or other logic.\n",
    "    # For example, removing rows with missing labels:\n",
    "    data = data.dropna(subset=['label'])\n",
    "    print(\"Missing label rows have been removed.\")\n",
    "else:\n",
    "    print(\"No missing labels found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of rows before removal for debugging and verification purposes.\n",
    "print(f\"Number of rows before removal of missing labels: {len(data)}\")\n",
    "\n",
    "# Remove rows from the DataFrame 'data' where the 'label' column contains missing values.\n",
    "data.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "# Verify and output the number of rows after removal to ensure that the operation was successful.\n",
    "print(f\"Number of rows after removal of missing labels: {len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the presence of rows with missing labels and log them if any.\n",
    "if not missing_label_rows.empty:\n",
    "    print(f\"Rows with missing labels:\\n{missing_label_rows}\")\n",
    "else:\n",
    "    # Log when no missing label rows are found, which is good for confirming data quality.\n",
    "    print(\"No missing label rows found.\")\n",
    "\n",
    "# After removing rows with missing labels, check if the DataFrame 'data' has become empty.\n",
    "if data.empty:\n",
    "    print(\"The DataFrame is empty after preprocessing.\")\n",
    "    # Depending on the application, you might want to handle an empty DataFrame specifically.\n",
    "    # For example, you might raise an exception or trigger an alert in a production environment.\n",
    "    raise ValueError(\"Critical error: Data processing resulted in an empty dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels: Convert categorical labels from strings to integers for further analysis or modeling.\n",
    "# This mapping is crucial for many machine learning algorithms that require numerical input.\n",
    "label_mapping = {'NONE': 0, 'FAVOR': 1, 'AGAINST': 2}  # Define a dictionary to map string labels to integers.\n",
    "\n",
    "# Apply the mapping to the 'label' column in the DataFrame 'data'.\n",
    "# This replaces the string labels in the 'label' column with their corresponding integer codes.\n",
    "data['label'] = data['label'].map(label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the encoding: Ensure that all categorical labels have been converted to the predefined numerical codes.\n",
    "unique_labels = data['label'].unique()  # Extract unique values from the 'label' column to check the encoding results.\n",
    "\n",
    "# Check if the unique labels in the dataset match the expected set of numerical codes {0, 1, 2}.\n",
    "if set(unique_labels) != {0, 1, 2}:\n",
    "    # Prepare an error message detailing the mismatch in expected and actual unique labels.\n",
    "    error_msg = f\"Labels are not correctly mapped. Found unique labels: {unique_labels}\"\n",
    "    \n",
    "    # Log the error message to help with debugging and ensuring the issue is recorded.\n",
    "    logging.error(error_msg)\n",
    "    \n",
    "    # Raise a ValueError to stop the execution and signal that the label encoding has issues.\n",
    "    raise ValueError(error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the encoding and suggest corrections if mismatches are found.\n",
    "unique_labels = data['label'].unique()\n",
    "expected_labels = {0, 1, 2}\n",
    "\n",
    "if set(unique_labels) != expected_labels:\n",
    "    error_msg = f\"Labels are not correctly mapped. Found unique labels: {unique_labels}, expected labels were {expected_labels}.\"\n",
    "    logging.error(error_msg)\n",
    "    \n",
    "    # Suggest checking the label_mapping dictionary or reviewing the source data for inconsistencies.\n",
    "    corrective_action = \"Please review the label_mapping dictionary and the source data for any inconsistencies.\"\n",
    "    logging.info(corrective_action)\n",
    "    \n",
    "    # Raise an error with both the problem and suggested corrective actions.\n",
    "    raise ValueError(f\"{error_msg} {corrective_action}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['text', 'label'])\n",
    "    return df\n",
    "\n",
    "def train_model(model, tokenizer, data, label_to_index):\n",
    "    train_encodings = tokenizer(data['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "    train_labels = [label_to_index[label] for label in data['label']]\n",
    "    train_dataset = TextDataset(train_encodings, train_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=3,              # number of training epochs\n",
    "        per_device_train_batch_size=8,   # batch size for training\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        compute_metrics=None\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"experiment_bert-base-uncased\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"bert-base-uncased-finetuned\"):\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(data['label'].unique()))\n",
    "\n",
    "    # Mapping labels to numeric indices for processing\n",
    "    label_to_index = {label: idx for idx, label in enumerate(data['label'].unique())}\n",
    "\n",
    "    # Log the model parameters\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, tokenizer, data, label_to_index)\n",
    "    \n",
    "    # Assuming you have a validation or test set, you would evaluate it here\n",
    "    # For demonstration, we assume 'data' contains some training data.\n",
    "    # evaluation results would typically go here\n",
    "\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Assuming you've defined TextDataset and load_data as before\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['text'], df['label'].astype(int)  # Ensure labels are integer\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)  # Set num_labels appropriately\n",
    "\n",
    "# Prepare data\n",
    "texts, labels = load_data(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv\")\n",
    "encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "dataset = TextDataset(encodings, labels)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory for model and logs\n",
    "    num_train_epochs=3,              # number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size for training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    compute_metrics=None  # Define compute_metrics function if needed\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['text', 'label'])\n",
    "    return df\n",
    "\n",
    "def train_model(model, tokenizer, data, label_to_index):\n",
    "    train_encodings = tokenizer(data['text'].tolist(), truncation=True, padding=True, max_length=512)\n",
    "    train_labels = [label_to_index[label] for label in data['label']]\n",
    "    train_dataset = TextDataset(train_encodings, train_labels)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',          # output directory\n",
    "        num_train_epochs=3,              # number of training epochs\n",
    "        per_device_train_batch_size=8,   # batch size for training\n",
    "        warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,               # strength of weight decay\n",
    "        logging_dir='./logs',            # directory for storing logs\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        compute_metrics=None\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv\"\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Set a new experiment name\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"experiment_distilbert-base-uncased\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"distilbert-base-uncased-finetuned\"):\n",
    "    model_name = \"distilbert-base-uncased\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(data['label'].unique()))\n",
    "\n",
    "    # Mapping labels to numeric indices for processing\n",
    "    label_to_index = {label: idx for idx, label in enumerate(data['label'].unique())}\n",
    "\n",
    "    # Log the model parameters\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "\n",
    "    # Train the model\n",
    "    train_model(model, tokenizer, data, label_to_index)\n",
    "    \n",
    "    # Here you would ideally have a separate evaluation phase with your validation/test set\n",
    "    # For now, this example assumes that the training function encompasses all necessary processes\n",
    "    # Log metrics to MLflow\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path, usecols=['text', 'label'])\n",
    "    # Assuming label mapping needs to be consistent with model's output\n",
    "    unique_labels = sorted(df['label'].unique())\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    df['label'] = df['label'].map(label_to_index)  # Map textual labels to indices\n",
    "    return df, label_to_index\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "    for batch in loader:\n",
    "        inputs = {key: val.to(model.device) for key, val in batch.items() if key != 'labels'}\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "file_path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv\"\n",
    "data, label_to_index = load_data(file_path)\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"experiment_bert-election2020-twitter-stance-biden\")\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=\"bert-election2020-twitter-stance-biden-evaluation\"):\n",
    "    model_name = \"kornosk/bert-election2020-twitter-stance-biden\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    encodings = tokenizer(data['text'].tolist(), truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    dataset = TextDataset(encodings, data['label'].tolist())\n",
    "\n",
    "    accuracy, precision, recall, f1 = evaluate_model(model, tokenizer, dataset)\n",
    "\n",
    "    mlflow.log_param(\"model_name\", model_name)\n",
    "    mlflow.log_metrics({\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "gr.load(\"models/kornosk/bert-election2020-twitter-stance-biden\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate the length of each sentence in the 'text' column to prepare data for histogram\n",
    "sentence_lengths = [len(s) for s in data.text]\n",
    "\n",
    "# Create a histogram plot to visualize the distribution of sentence lengths\n",
    "plt.figure(figsize=(10, 5))  # Set the size of the plot\n",
    "sns.histplot(sentence_lengths, bins=100, color='skyblue', edgecolor='black')  # Create a histogram with 100 bins\n",
    "plt.title('Sentence Length Distribution')  # Set the title of the histogram\n",
    "plt.xlabel('Sentence Length')  # Label the x-axis as 'Sentence Length'\n",
    "plt.ylabel('Frequency')  # Label the y-axis as 'Frequency'\n",
    "plt.grid(True)  # Enable grid to improve readability\n",
    "plt.show()  # Display the plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean and median sentence length\n",
    "mean_len = np.mean(sentence_lengths)\n",
    "median_len = np.median(sentence_lengths)\n",
    "\n",
    "# Add vertical lines for mean and median to the plot\n",
    "plt.axvline(mean_len, color='r', linestyle='--', linewidth=1, label=f'Mean: {mean_len:.2f}')\n",
    "plt.axvline(median_len, color='g', linestyle='-', linewidth=1, label=f'Median: {median_len:.2f}')\n",
    "\n",
    "# Add a legend to the plot to explain the additional lines\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Missing Values\n",
    "missing_values = data.isnull().sum()\n",
    "missing_percentage = (missing_values / len(data)) * 100\n",
    "missing_df = pd.DataFrame({'Feature': missing_values.index, 'MissingValues': missing_values, 'Percentage': missing_percentage})\n",
    "# Filter out features with no missing values\n",
    "missing_df = missing_df[missing_df['MissingValues'] > 0].sort_values('Percentage', ascending=False)\n",
    "# Plotting missing values (if there are any)\n",
    "if not missing_df.empty:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Percentage', y='Feature', data=missing_df)\n",
    "    plt.title('Percentage of Missing Values per Feature')\n",
    "    plt.xlabel('Percentage')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values found in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import re\n",
    "import logging\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def preprocess_and_visualize_step(data: pd.DataFrame) -> Output(processed_data=pd.DataFrame, figures=list):\n",
    "    \"\"\"\n",
    "    Step to preprocess and visualize data.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Dataframe to be processed.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed data.\n",
    "        list: List of matplotlib figure objects.\n",
    "    \"\"\"\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Function to clean text data.\n",
    "        \"\"\"\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        text = re.sub(r'@\\S+|#\\S+', '', text)\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        text = text.lower().strip()\n",
    "        tokens = text.split()\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    expected_columns = {'text', 'label'}\n",
    "    if not expected_columns.issubset(data.columns):\n",
    "        missing_cols = expected_columns - set(data.columns)\n",
    "        error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    print(\"Preprocessing the data...\")\n",
    "    # Apply the cleaning function to the text column\n",
    "    data['text'] = data['text'].apply(clean_text)\n",
    "\n",
    "    # Drop rows where 'label' is NaN\n",
    "    data.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "    # Encode labels\n",
    "    label_mapping = {'NONE': 0, 'FAVOR': 1, 'AGAINST': 2}\n",
    "    data['label'] = data['label'].map(label_mapping)\n",
    "\n",
    "    # Validate the encoding\n",
    "    unique_labels = data['label'].unique()\n",
    "    if set(unique_labels) != {0, 1, 2}:\n",
    "        error_msg = f\"Labels are not correctly mapped. Found unique labels: {unique_labels}\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "\n",
    "    # Visualization of sentence length\n",
    "    fig1 = plt.figure(figsize=(10, 5))\n",
    "    sns.histplot([len(s) for s in data['text']], bins=100)\n",
    "    plt.title('Sentence Length')\n",
    "    plt.show()\n",
    "\n",
    "    # Visualization of Missing Values\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / len(data)) * 100\n",
    "    missing_df = pd.DataFrame({'Feature': missing_values.index, 'MissingValues': missing_values, 'Percentage': missing_percentage})\n",
    "    missing_df = missing_df[missing_df['MissingValues'] > 0].sort_values('Percentage', ascending=False)\n",
    "\n",
    "    if not missing_df.empty:\n",
    "        fig2 = plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Percentage', y='Feature', data=missing_df)\n",
    "        plt.title('Percentage of Missing Values per Feature')\n",
    "        plt.xlabel('Percentage')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No missing values found in the dataset.\")\n",
    "    \n",
    "    return data, [fig1, fig2] if not missing_df.empty else [fig1]\n",
    "\n",
    "# This setup assumes logging is configured elsewhere in your application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Distribution Visualization\n",
    "# Check if the 'label' column exists in the DataFrame to ensure the relevant data is available for visualization.\n",
    "if 'label' in data.columns:\n",
    "    plt.figure(figsize=(8, 4))  # Set the size of the plot for clear visibility\n",
    "    sns.countplot(x='label', data=data, palette='viridis')  # Create a count plot for the 'label' column with a visually appealing color palette\n",
    "    plt.title('Distribution of Labels')  # Title of the plot, describing what is being visualized\n",
    "    plt.xlabel('Label')  # Label for the x-axis, indicating what the values represent\n",
    "    plt.ylabel('Count')  # Label for the y-axis, indicating the quantity being measured\n",
    "    plt.xticks(rotation=45)  # Rotate the labels on the x-axis for better readability if they are long or numerous\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)  # Add a grid to the y-axis to enhance readability of the count values\n",
    "    plt.show()  # Display the plot\n",
    "else:\n",
    "    # If the 'label' column is not found, output a message and skip the visualization step.\n",
    "    print(\"Label column not found, skipping label distribution visualization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamically adjust the rotation of x-axis labels based on the number of unique labels\n",
    "label_count = data['label'].nunique()\n",
    "rotation_angle = 45 if label_count > 5 else 0  # Rotate only if there are more than 5 labels to avoid clutter\n",
    "\n",
    "# Apply the rotation\n",
    "plt.xticks(rotation=rotation_angle)\n",
    "\n",
    "# Dynamically adjust plot size based on label count\n",
    "fig_width = max(8, label_count)  # Ensure the plot width is adequate to display all labels without overlapping\n",
    "plt.figure(figsize=(fig_width, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for your test and validation sets\n",
    "test_size = 0.2\n",
    "validation_size = 0.1\n",
    "\n",
    "# Initial split to separate out the test set\n",
    "train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "# Calculate the adjusted validation size based on the remaining data after test split\n",
    "adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "# Split the remaining data into training and validation sets\n",
    "train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics for the training dataset\n",
    "# TensorFlow Data Validation (TFDV) is used here to compute a comprehensive set of statistics\n",
    "# for the training dataset stored in the DataFrame 'train_data'. These statistics are essential\n",
    "# for understanding the distribution of data, detecting anomalies, and making informed decisions\n",
    "# about data preprocessing and feature engineering steps.\n",
    "\n",
    "# Generate statistics from the DataFrame\n",
    "train_stats = tfdv.generate_statistics_from_dataframe(train_data)\n",
    "\n",
    "# The generated statistics include counts, means, variances, and other descriptive metrics\n",
    "# that help in visualizing and understanding data distributions and patterns. This step is crucial\n",
    "# for ensuring data quality and preparing the dataset for effective machine learning model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(train_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_stats = tfdv.generate_statistics_from_dataframe(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats = tfdv.generate_statistics_from_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.visualize_statistics(test_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer schema from training data\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "print(\"Schema inferred from the testing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the schema to a file\n",
    "schema_file = 'schema.pbtxt'\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "print(f\"Schema saved to {schema_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pairs of dataset names and their respective statistics.\n",
    "# This loop is designed to validate statistics for multiple datasets (Validation and Test) against a common schema.\n",
    "for dataset_name, dataset_stats in [('Validation', val_stats), ('Test', test_stats)]:\n",
    "    # Print a message to indicate which dataset is currently being validated. This helps in tracking the process flow.\n",
    "    print(f\"Validating {dataset_name} data...\")\n",
    "    \n",
    "    # Use TFDV to validate the dataset statistics against the predefined schema.\n",
    "    # The schema helps ensure that the dataset adheres to expected formats and value ranges.\n",
    "    anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "    \n",
    "    # Check if there are any anomalies found in the dataset.\n",
    "    if anomalies.anomaly_info:\n",
    "        # If anomalies are detected, print a message and display the anomalies.\n",
    "        print(f\"Anomalies found in {dataset_name} data:\")\n",
    "        tfdv.display_anomalies(anomalies)\n",
    "    else:\n",
    "        # If no anomalies are detected, print a message indicating the dataset is clean.\n",
    "        print(f\"No anomalies detected in {dataset_name} data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, filename='data_validation_log.txt', filemode='w',\n",
    "                    format='%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "for dataset_name, dataset_stats in [('Validation', val_stats), ('Test', test_stats)]:\n",
    "    print(f\"Validating {dataset_name} data...\")\n",
    "    anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "    if anomalies.anomaly_info:\n",
    "        print(f\"Anomalies found in {dataset_name} data:\")\n",
    "        tfdv.display_anomalies(anomalies)\n",
    "        # Log anomalies to a file\n",
    "        logging.error(f\"Anomalies found in {dataset_name} data: {anomalies.anomaly_info}\")\n",
    "        # Here you could add code to handle anomalies based on their types, such as re-cleaning data or alerting teams.\n",
    "    else:\n",
    "        print(f\"No anomalies detected in {dataset_name} data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the statistics of the training dataset against a predefined schema using TFDV.\n",
    "# This step is essential for ensuring that the data conforms to expected formats, value ranges, and other schema rules,\n",
    "# which are critical for maintaining data quality and preventing issues during model training.\n",
    "\n",
    "# Perform the validation and store any detected anomalies.\n",
    "anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema)\n",
    "\n",
    "# Display the anomalies using TFDV's built-in function.\n",
    "# This function visualizes anomalies in a user-friendly format, making it easier to interpret and address them.\n",
    "# Anomalies might include unexpected missing values, incorrect data types, or data distribution issues that do not comply with the schema.\n",
    "tfdv.display_anomalies(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup logging to capture output in a log file\n",
    "logging.basicConfig(level=logging.INFO, filename='anomaly_log.txt', filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Validate and log anomalies\n",
    "anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema)\n",
    "if anomalies.anomaly_info:  # Check if there are any anomalies\n",
    "    logging.info(f\"Anomalies detected: {anomalies.anomaly_info}\")\n",
    "    # Save the detailed anomaly info to a file or a database for further investigation if required\n",
    "    with open('detailed_anomalies.txt', 'w') as f:\n",
    "        f.write(str(anomalies.anomaly_info))\n",
    "\n",
    "# Display anomalies visually\n",
    "tfdv.display_anomalies(anomalies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_data_validation as tfdv\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def split_and_validate_data_step(data: pd.DataFrame) -> Output(train_data=pd.DataFrame, val_data=pd.DataFrame, test_data=pd.DataFrame, schema=object):\n",
    "    \"\"\"\n",
    "    Step to split the data into training, validation, and test sets and validate them.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The full dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Training data.\n",
    "        pd.DataFrame: Validation data.\n",
    "        pd.DataFrame: Test data.\n",
    "        object: TFDV schema object.\n",
    "    \"\"\"\n",
    "    # Data Distribution Visualization\n",
    "    if 'label' in data.columns:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.countplot(x='label', data=data)\n",
    "        plt.title('Distribution of Labels')\n",
    "        plt.xlabel('Label')\n",
    "        plt.ylabel('Count')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Label column not found, skipping label distribution visualization.\")\n",
    "\n",
    "    # Define the size for your test and validation sets\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.1\n",
    "\n",
    "    # Initial split to separate out the test set\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Calculate the adjusted validation size based on the remaining data after test split\n",
    "    adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "    # Split the remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)\n",
    "\n",
    "    # Generate statistics and infer schema\n",
    "    train_stats = tfdv.generate_statistics_from_dataframe(train_data)\n",
    "    val_stats = tfdv.generate_statistics_from_dataframe(val_data)\n",
    "    test_stats = tfdv.generate_statistics_from_dataframe(test_data)\n",
    "    schema = tfdv.infer_schema(statistics=train_stats)\n",
    "    print(\"Schema inferred from the training data.\")\n",
    "    tfdv.display_schema(schema=schema)\n",
    "\n",
    "    # Validate statistics against the schema\n",
    "    for dataset_name, dataset_stats in [('Validation', val_stats), ('Test', test_stats)]:\n",
    "        print(f\"Validating {dataset_name} data...\")\n",
    "        anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "        if anomalies.anomaly_info:\n",
    "            print(f\"Anomalies found in {dataset_name} data:\")\n",
    "            tfdv.display_anomalies(anomalies)\n",
    "        else:\n",
    "            print(f\"No anomalies detected in {dataset_name} data.\")\n",
    "\n",
    "    # Also validate training data\n",
    "    anomalies = tfdv.validate_statistics(statistics=train_stats, schema=schema)\n",
    "    tfdv.display_anomalies(anomalies)\n",
    "\n",
    "    return train_data, val_data, test_data, schema\n",
    "\n",
    "# Ensure tensorflow_data_validation is installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM_PATH)\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Connecting to the Cassandra Database...\")\n",
    "cluster = Cluster(contact_points=['127.0.0.1'],port=9042,load_balancing_policy=DCAwareRoundRobinPolicy())\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('keyspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import BertTokenizer\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "from cassandra import ConsistencyLevel\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Cassandra session to use the specified keyspace.\n",
    "# This is necessary before performing any operations that involve reading from or writing to tables within this keyspace.\n",
    "session.set_keyspace('keyspace')\n",
    "\n",
    "# Assuming that cleaning means deleting specific or all entries from a table or multiple tables within the keyspace.\n",
    "# Below are examples of how to clean data from a table named 'features'. Adjust the table name and conditions as necessary.\n",
    "\n",
    "# Prepare a CQL query to delete all entries from the 'features' table.\n",
    "# Warning: This will remove all data from the table. Use with caution.\n",
    "delete_query = 'TRUNCATE TABLE features'\n",
    "\n",
    "# Execute the deletion query to clean the table.\n",
    "session.execute(delete_query)\n",
    "\n",
    "# Print a confirmation message once the table has been cleaned.\n",
    "print(\"The 'features' table in the 'keyspace' keyspace has been cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Cassandra session to use the specified keyspace.\n",
    "session.set_keyspace('keyspace')\n",
    "\n",
    "# Retrieve the list of all tables in the keyspace.\n",
    "# The 'metadata' attribute of the session cluster provides access to the schema metadata which includes table names.\n",
    "table_names = session.cluster.metadata.keyspaces['keyspace'].tables.keys()\n",
    "\n",
    "# Loop through each table name to fetch and print its contents.\n",
    "for table_name in table_names:\n",
    "    # Prepare a CQL query to select all records from the current table.\n",
    "    query = f'SELECT * FROM {table_name}'\n",
    "\n",
    "    # Execute the query and fetch the results.\n",
    "    results = session.execute(query)\n",
    "\n",
    "    # Print the name of the table and its contents.\n",
    "    # This assumes that the result of the query returns an iterable of rows.\n",
    "    print(f\"Contents of table '{table_name}':\")\n",
    "    for row in results:\n",
    "        print(row)\n",
    "    print(\"\\n\")  # Print a newline for better readability between table outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "from cassandra.query import BatchStatement, ConsistencyLevel\n",
    "from cassandra import ConsistencyLevel\n",
    "\n",
    "# Assuming 'session' and 'tokenizer' are predefined\n",
    "# Assuming 'data' is a DataFrame loaded from your .csv file\n",
    "\n",
    "# Prepare the insert statement\n",
    "insert_statement = session.prepare(\n",
    "    'INSERT INTO \"keyspace\".features (id, tokens, label) VALUES (?, ?, ?)'\n",
    ")\n",
    "\n",
    "# Create a batch statement\n",
    "batch = BatchStatement(consistency_level=ConsistencyLevel.ONE)\n",
    "batch_size_limit = 4\n",
    "\n",
    "# Iterate over rows in the DataFrame\n",
    "for index, row in data.iterrows():\n",
    "    row_id = uuid.uuid4()\n",
    "\n",
    "    # Tokenize text and ensure tokens are integers\n",
    "    tokens = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "    if not all(isinstance(token, int) for token in tokens):\n",
    "        continue  # Skip this row if tokens are not properly formatted\n",
    "\n",
    "    label = str(row['label'])  # Convert label to string\n",
    "\n",
    "    # Add data to batch\n",
    "    batch.add(insert_statement, (row_id, tokens, label))\n",
    "\n",
    "    # Execute batch if it reaches the size limit\n",
    "    if len(batch) >= batch_size_limit:\n",
    "        session.execute(batch)\n",
    "        batch.clear()\n",
    "\n",
    "# Execute any remaining statements in the batch\n",
    "if len(batch) > 0:\n",
    "    session.execute(batch)\n",
    "\n",
    "print(\"Feature engineering and data insertion completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Setup basic logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Inside your batch processing loop\n",
    "try:\n",
    "    # Existing processing code...\n",
    "    session.execute(batch)\n",
    "    batch.clear()\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed to execute batch: {e}\")\n",
    "    # Optionally clear the batch to avoid retrying the same failing batch\n",
    "    batch.clear()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all table names in the keyspace\n",
    "# Execute a CQL query on the system_schema.tables to retrieve all table names for a specific keyspace.\n",
    "# Replace \"keyspace\" with the actual name of your keyspace.\n",
    "rows = session.execute(\"SELECT table_name FROM system_schema.tables WHERE keyspace_name = %s\", [\"keyspace\"])\n",
    "\n",
    "# Extract the table names from the query results.\n",
    "# This creates a list of table names by iterating over the rows of the result set.\n",
    "table_names = [row[0] for row in rows]\n",
    "\n",
    "# Iterate through each table name fetched from the keyspace.\n",
    "for table_name in table_names:\n",
    "    # Print the name of the table to indicate which table's contents are being displayed.\n",
    "    print(f\"Contents of table {table_name}:\")\n",
    "\n",
    "    # Execute a SELECT query to fetch all rows from the current table.\n",
    "    rows = session.execute(f\"SELECT * FROM {table_name}\")\n",
    "\n",
    "    # Iterate through each row in the result set and print it.\n",
    "    # This prints each row's data, showing the contents of the table.\n",
    "    for row in rows:\n",
    "        print(row)\n",
    "    \n",
    "    # Print a newline after each table's contents for better readability in the output.\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SELECT * FROM \"keyspace\".features LIMIT 10;\n",
    "#SELECT COUNT(*) FROM \"keyspace\".features;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy\n",
    "from cassandra.query import BatchStatement\n",
    "from cassandra import ConsistencyLevel\n",
    "import uuid\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def tokenize_and_insert_data_step(data: pd.DataFrame, PRETRAINED_LM_PATH: str) -> Output(insertion_success=bool):\n",
    "    \"\"\"\n",
    "    Step to tokenize text data and insert into Cassandra database.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): The dataframe containing text data.\n",
    "        PRETRAINED_LM_PATH (str): Path to the pretrained language model for tokenization.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if data insertion was successful, False otherwise.\n",
    "    \"\"\"\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM_PATH)\n",
    "\n",
    "    # Connect to the Cassandra Database\n",
    "    print(\"Connecting to the Cassandra Database...\")\n",
    "    cluster = Cluster(contact_points=['127.0.0.1'], port=9042, load_balancing_policy=DCAwareRoundRobinPolicy())\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('keyspace')\n",
    "\n",
    "    # Preparing the insert statement\n",
    "    insert_statement = session.prepare(\n",
    "        'INSERT INTO \"keyspace\".features (id, tokens, label) VALUES (?, ?, ?)'\n",
    "    )\n",
    "    batch = BatchStatement(consistency_level=ConsistencyLevel.ONE)\n",
    "    batch_size_limit = 16\n",
    "\n",
    "    # Tokenization and data insertion\n",
    "    try:\n",
    "        for index, row in data.iterrows():\n",
    "            row_id = uuid.uuid4()\n",
    "            tokens = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "            label = str(row['label'])\n",
    "            batch.add(insert_statement, (row_id, tokens, label))\n",
    "\n",
    "            if len(batch) >= batch_size_limit:\n",
    "                session.execute(batch)\n",
    "                batch.clear()\n",
    "\n",
    "        if len(batch) > 0:\n",
    "            session.execute(batch)\n",
    "        \n",
    "        # Display success message\n",
    "        print(\"Feature engineering and data insertion completed successfully.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during data insertion: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        session.shutdown()\n",
    "\n",
    "# Ensure that the necessary Python libraries are installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Connecting to the Cassandra Database...\")\n",
    "cluster = Cluster(contact_points=['127.0.0.1'], port=9042)\n",
    "session = cluster.connect()\n",
    "session.set_keyspace('keyspace')\n",
    "\n",
    "# Query to retrieve data from Cassandra\n",
    "query = \"SELECT id, tokens, label FROM features;\"\n",
    "rows = session.execute(query)\n",
    "\n",
    "# Convert the data to a Pandas DataFrame\n",
    "data = pd.DataFrame(list(rows))\n",
    "\n",
    "# Assuming 'tokens' are your features and 'label' is the target variable\n",
    "# Format data for training\n",
    "X = data['tokens'].tolist()  # Feature set\n",
    "y = data['label'].tolist()   # Labels\n",
    "\n",
    "# At this point, X and y can be used in a machine learning model training process.\n",
    "# Example: Using these in a PyTorch or TensorFlow training pipeline.\n",
    "\n",
    "print(\"Data loading completed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data creation (replace or omit this if 'data' is already defined)\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'data' DataFrame already has a 'tokens' column with tokenized data\n",
    "# For example:\n",
    "d = pd.DataFrame({\n",
    "    'text': ['Hello world', 'Testing 123', 'Sample text'],\n",
    "    'tokens': [[101, 12345, 102], [101, 67890, 102], [101, 54321, 102]],\n",
    "    'label': ['greeting', 'test', 'example']\n",
    "})\n",
    "\n",
    "# Extracting tokens column to a list\n",
    "x = d['tokens'].tolist()\n",
    "\n",
    "# Printing the list X\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have already loaded your data into X (features) and y (labels)\n",
    "\n",
    "# Splitting the dataset into training and temporary sets (combining validation and test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "# Further splitting the temporary set into validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "# Now, X_train, y_train are your training data\n",
    "# X_val, y_val are your validation data\n",
    "# X_test, y_test are your test data\n",
    "\n",
    "# You can proceed with using these datasets for training and evaluating your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from cassandra.cluster import Cluster\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def load_and_split_data_step() -> Output(X_train=list, X_val=list, X_test=list, y_train=list, y_val=list, y_test=list):\n",
    "    \"\"\"\n",
    "    Step to load data from a Cassandra database, convert it to a suitable format, and split into training, validation, and test sets.\n",
    "\n",
    "    Returns:\n",
    "        list: Training features.\n",
    "        list: Validation features.\n",
    "        list: Test features.\n",
    "        list: Training labels.\n",
    "        list: Validation labels.\n",
    "        list: Test labels.\n",
    "    \"\"\"\n",
    "    print(\"Connecting to the Cassandra Database...\")\n",
    "    cluster = Cluster(contact_points=['127.0.0.1'], port=9042)\n",
    "    session = cluster.connect()\n",
    "    session.set_keyspace('keyspace')\n",
    "\n",
    "    # Query to retrieve data\n",
    "    query = \"SELECT id, tokens, label FROM features;\"\n",
    "    rows = session.execute(query)\n",
    "\n",
    "    # Convert the data to a Pandas DataFrame\n",
    "    data = pd.DataFrame(list(rows))\n",
    "\n",
    "    # Assuming 'tokens' are features and 'label' is the target\n",
    "    X = data['tokens'].tolist()  # Feature set\n",
    "    y = data['label'].tolist()   # Labels\n",
    "\n",
    "    print(\"Data loading completed successfully.\")\n",
    "\n",
    "    # Splitting the dataset into training and temporary sets (combining validation and test)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "    # Further splitting the temporary set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Validation set size: {len(X_val)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Note: Ensure Cassandra driver is installed and correctly configured in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "\n",
    "# Define a function for random search hyperparameter optimization\n",
    "def random_search(model, param_space, num_iterations, data_loader, validation_loader, device):\n",
    "    # Initialize variables to track the best score and corresponding parameters\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop through the specified number of iterations for hyperparameter search\n",
    "    for iteration in range(num_iterations):\n",
    "        # Randomly sample hyperparameters from the provided parameter space\n",
    "        params = {key: val.rvs() for key, val in param_space.items()}\n",
    "        print(f\"Iteration {iteration + 1}, using parameters: {params}\")\n",
    "\n",
    "        # Update the model with the sampled hyperparameters\n",
    "        optimizer = AdamW(model.parameters(), lr=params['learning_rate'], eps=1e-8)\n",
    "        total_steps = len(data_loader) * params['epochs']\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        # Loop through each epoch for training\n",
    "        for epoch in range(params['epochs']):\n",
    "            # Set the model to training mode\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            # Loop through each batch in the training data loader\n",
    "            for step, batch in enumerate(data_loader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                # Reset gradients, perform forward pass, calculate loss, perform backward pass, and update weights\n",
    "                model.zero_grad()\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            # Set the model to evaluation mode\n",
    "            model.eval()\n",
    "            total_eval_accuracy = 0\n",
    "\n",
    "            # Loop through each batch in the validation data loader\n",
    "            for batch in validation_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                # Perform forward pass without gradient calculation\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "                # Calculate accuracy\n",
    "                logits = outputs.logits\n",
    "                logits = logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            # Calculate average validation accuracy for the epoch\n",
    "            avg_val_accuracy = total_eval_accuracy / len(validation_loader)\n",
    "\n",
    "        # Calculate the evaluation metric (accuracy here)\n",
    "        eval_metric = avg_val_accuracy\n",
    "\n",
    "        # Update the best score and parameters if the current iteration achieves a higher score\n",
    "        if eval_metric > best_score:\n",
    "            best_score = eval_metric\n",
    "            best_params = params\n",
    "\n",
    "    # Return the best parameters and corresponding score\n",
    "    return best_params, best_score\n",
    "\n",
    "# Function to calculate the accuracy of model predictions\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "from zenml.steps import step, Output\n",
    "from scipy.stats import loguniform, randint\n",
    "\n",
    "@step\n",
    "def hyperparameter_tuning_step(model: torch.nn.Module, data_loader: torch.utils.data.DataLoader, validation_loader: torch.utils.data.DataLoader, device: str, num_iterations: int) -> Output(best_params=dict, best_score=float):\n",
    "    \"\"\"\n",
    "    Step to perform random search hyperparameter tuning on a model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to tune.\n",
    "        data_loader (DataLoader): DataLoader for training data.\n",
    "        validation_loader (DataLoader): DataLoader for validation data.\n",
    "        device (str): Device to perform computation on.\n",
    "        num_iterations (int): Number of iterations for random search.\n",
    "\n",
    "    Returns:\n",
    "        dict: Best hyperparameter set found.\n",
    "        float: Best evaluation metric score.\n",
    "    \"\"\"\n",
    "    # Define the hyperparameter space\n",
    "    param_space = {\n",
    "        'learning_rate': loguniform(1e-6, 1e-4),\n",
    "        'epochs': randint(2, 10)\n",
    "    }\n",
    "\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    # Function to calculate the accuracy of predictions vs labels\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    for iteration in range(num_iterations):\n",
    "        # Randomly sample parameters\n",
    "        params = {key: val.rvs() for key, val in param_space.items()}\n",
    "        print(f\"Iteration {iteration + 1}, using parameters: {params}\")\n",
    "\n",
    "        # Update model with sampled hyperparameters\n",
    "        optimizer = AdamW(model.parameters(), lr=params['learning_rate'], eps=1e-8)\n",
    "        total_steps = len(data_loader) * params['epochs']\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        for epoch in range(params['epochs']):\n",
    "            # Training loop\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for step, batch in enumerate(data_loader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                model.zero_grad()\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            # Validation loop\n",
    "            model.eval()\n",
    "            total_eval_accuracy = 0\n",
    "            for batch in validation_loader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "                logits = outputs.logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "            avg_val_accuracy = total_eval_accuracy / len(validation_loader)\n",
    "\n",
    "        # Update best score and parameters if the current score is higher\n",
    "        if avg_val_accuracy > best_score:\n",
    "            best_score = avg_val_accuracy\n",
    "            best_params = params\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# Ensure that all necessary packages, including scipy for distributions and any necessary model and data utilities, are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden')\n",
    "\n",
    "# Encoding labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# BERT Input Formatting\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for sent in X:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors\n",
    "                   )\n",
    "    \n",
    "    # Add the encoded sentence to the list\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "    # And its attention mask (simply differentiates padding from non-padding)\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(y_encoded)\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = torch.utils.data.SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "# Initialize BertForSequenceClassification\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\",\n",
    "    num_labels = len(label_encoder.classes_) # The number of output labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def prepare_for_training_step(X: list, y: list) -> Output(train_dataloader=DataLoader, validation_dataloader=DataLoader, model=BertForSequenceClassification):\n",
    "    \"\"\"\n",
    "    Step to prepare data for training using BERT model, including tokenization and setting up dataloaders.\n",
    "\n",
    "    Args:\n",
    "        X (list): List of text data (sentences).\n",
    "        y (list): List of labels.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: PyTorch DataLoader for training data.\n",
    "        DataLoader: PyTorch DataLoader for validation data.\n",
    "        BertForSequenceClassification: Pretrained BERT model configured for sequence classification.\n",
    "    \"\"\"\n",
    "    # Initialize the BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden')\n",
    "\n",
    "    # Encoding labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "    # BERT Input Formatting\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for sent in X:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            sent,                     \n",
    "            add_special_tokens=True,  \n",
    "            max_length=64,           \n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,  \n",
    "            return_tensors='pt',     \n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert the lists into tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(y_encoded)\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "    train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=2018, test_size=0.1)\n",
    "\n",
    "    # Create DataLoader for training set\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = torch.utils.data.RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)\n",
    "\n",
    "    # Create DataLoader for validation set\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = torch.utils.data.SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=32)\n",
    "\n",
    "    # Initialize BertForSequenceClassification\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\",\n",
    "        num_labels=len(label_encoder.classes_)\n",
    "    )\n",
    "\n",
    "    return train_dataloader, validation_dataloader, model\n",
    "\n",
    "# Note: This step assumes you have the required packages and PyTorch environment configured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# BERT requires a special optimizer and a learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function for formatting elapsed times\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Training Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # Training\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "    # Validation\n",
    "    print(\"\\nRunning Validation...\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        \n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_eval_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a separate test dataset named 'test_dataset' and a function to evaluate model performance\n",
    "# Import necessary libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Define or import the necessary libraries and functions\n",
    "\n",
    "# Define or import the test dataset and create a DataLoader\n",
    "# test_dataset = ...\n",
    "# test_loader = ...\n",
    "\n",
    "# Function to evaluate model performance on the test dataset\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        _, predicted_labels = torch.max(logits, dim=1)\n",
    "\n",
    "        all_labels.extend(b_labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    test_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    class_report = classification_report(all_labels, all_predictions)\n",
    "\n",
    "    return test_accuracy, class_report\n",
    "\n",
    "# Evaluate the trained model on the test dataset\n",
    "test_accuracy, class_report = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Print the test accuracy and classification report\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def train_model_step(train_dataloader: DataLoader, validation_dataloader: DataLoader, model: torch.nn.Module) -> Output(model=torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Step to train a BERT model using PyTorch, including training and validation phases.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader): DataLoader for training data.\n",
    "        validation_dataloader (DataLoader): DataLoader for validation data.\n",
    "        model (torch.nn.Module): Pretrained BERT model for sequence classification.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Trained model.\n",
    "    \"\"\"\n",
    "    # Set the seed value all over the place to make this reproducible\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # BERT requires a special optimizer and a learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 10\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Function to calculate the accuracy of our predictions vs labels\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    # Function for formatting elapsed times\n",
    "    def format_time(elapsed):\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "    # Training Loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n",
    "        print('Training...')\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and step != 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}.')\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"  Average training loss: {avg_train_loss:.2f}\")\n",
    "\n",
    "        # Validation phase\n",
    "        print(\"\\nRunning Validation...\")\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        print(f\"  Accuracy: {avg_val_accuracy:.2f}\")\n",
    "        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Ensure all necessary packages and configurations are set up in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have a function to get predictions from your model\n",
    "def get_predictions(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "\n",
    "    return np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)\n",
    "\n",
    "# Get predictions for the validation set\n",
    "predictions, true_labels = get_predictions(model, validation_dataloader)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def evaluate_model_step(model: torch.nn.Module, validation_dataloader: DataLoader, device: str) -> Output(report=str):\n",
    "    \"\"\"\n",
    "    Step to evaluate a BERT model using a validation dataloader, and visualize the results with a confusion matrix.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model for evaluation.\n",
    "        validation_dataloader (DataLoader): DataLoader containing validation data.\n",
    "        device (str): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        str: Classification report as a string.\n",
    "    \"\"\"\n",
    "    # Function to get predictions from the model\n",
    "    def get_predictions(model, dataloader):\n",
    "        model.eval()\n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "            logits = outputs.logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "\n",
    "        return np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)\n",
    "\n",
    "    # Get predictions for the validation set\n",
    "    predictions, true_labels = get_predictions(model, validation_dataloader)\n",
    "    predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Generate classification report\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    print(report)\n",
    "\n",
    "    # Generate and plot confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "    return report\n",
    "\n",
    "# Note: Make sure all dependencies (like matplotlib, seaborn, sklearn) are installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"BERT Stance Detection\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", 2e-5)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", train_dataloader.batch_size)\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # BERT requires a special optimizer and a learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 10\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Function to calculate the accuracy of our predictions vs labels\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    # Function for formatting elapsed times\n",
    "    def format_time(elapsed):\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "    # Training Loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # Training\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "        # Validation\n",
    "        print(\"\\nRunning Validation...\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Calculate precision, recall, and F1-score\n",
    "        predictions, true_labels = get_predictions(model, validation_dataloader) # Assuming get_predictions function\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch_i)\n",
    "\n",
    "        # Log precision, recall, and F1-score for each class\n",
    "        for label, metrics in report.items():\n",
    "            if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                mlflow.log_metric(f\"class_{label}_precision\", metrics[\"precision\"], step=epoch_i)\n",
    "                mlflow.log_metric(f\"class_{label}_recall\", metrics[\"recall\"], step=epoch_i)\n",
    "                mlflow.log_metric(f\"class_{label}_f1-score\", metrics[\"f1-score\"], step=epoch_i)\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch_i)\n",
    "        # Log macro and weighted average of precision, recall, and F1-score\n",
    "        mlflow.log_metric(\"macro_avg_precision\", report[\"macro avg\"][\"precision\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"macro_avg_recall\", report[\"macro avg\"][\"recall\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"macro_avg_f1-score\", report[\"macro avg\"][\"f1-score\"], step=epoch_i)\n",
    "\n",
    "        mlflow.log_metric(\"weighted_avg_precision\", report[\"weighted avg\"][\"precision\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"weighted_avg_recall\", report[\"weighted avg\"][\"recall\"], step=epoch_i)\n",
    "    mlflow.log_metric(\"weighted_avg_f1-score\", report[\"weighted avg\"][\"f1-score\"], step=epoch_i)\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "@step\n",
    "def train_and_log_with_mlflow_step(train_dataloader: DataLoader, validation_dataloader: DataLoader, model: torch.nn.Module, device: str) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    Step to train a model and log training/validation metrics and parameters to MLflow.\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader): DataLoader for training data.\n",
    "        validation_dataloader (DataLoader): DataLoader for validation data.\n",
    "        model (torch.nn.Module): The model to be trained and evaluated.\n",
    "        device (str): Device to perform computation on.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained model.\n",
    "    \"\"\"\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "    mlflow.set_experiment(\"BERT Stance Detection\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"learning_rate\", 2e-5)\n",
    "        mlflow.log_param(\"epochs\", 10)\n",
    "        mlflow.log_param(\"batch_size\", train_dataloader.batch_size)\n",
    "\n",
    "        seed_val = 42\n",
    "        random.seed(seed_val)\n",
    "        np.random.seed(seed_val)\n",
    "        torch.manual_seed(seed_val)\n",
    "        torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "        epochs = 10\n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "        model.to(device)\n",
    "\n",
    "        def flat_accuracy(preds, labels):\n",
    "            pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "            labels_flat = labels.flatten()\n",
    "            return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "        def format_time(elapsed):\n",
    "            return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "        for epoch_i in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            t0 = time.time()\n",
    "\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                if step % 40 == 0 and not step == 0:\n",
    "                    elapsed = format_time(time.time() - t0)\n",
    "                    print(f'  Batch {step} of {len(train_dataloader)}. Elapsed: {elapsed}.')\n",
    "\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                model.zero_grad()\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "            avg_train_loss = total_loss / len(train_dataloader)\n",
    "            print(f\"Average training loss: {avg_train_loss:.2f}\")\n",
    "            mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch_i)\n",
    "\n",
    "            model.eval()\n",
    "            total_eval_accuracy = 0\n",
    "            total_eval_loss = 0\n",
    "            eval_steps = 0\n",
    "\n",
    "            for batch in validation_dataloader:\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_eval_loss += loss.item()\n",
    "                logits = outputs.logits.detach().cpu().numpy()\n",
    "                label_ids = b_labels.to('cpu').numpy()\n",
    "                total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "                eval_steps += 1\n",
    "\n",
    "            avg_val_accuracy = total_eval_accuracy / eval_steps\n",
    "            avg_val_loss = total_eval_loss / eval_steps\n",
    "            print(f\"Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "            print(f\"Validation Loss: {avg_val_loss:.2f}\")\n",
    "            mlflow.log_metric(\"avg_val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "            mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch_i)\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        return model\n",
    "\n",
    "# Make sure MLflow, PyTorch, and all other required packages are installed in your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lengths = data['tokens'].apply(len)\n",
    "sns.histplot(token_lengths)\n",
    "plt.title('Token Length Distribution')\n",
    "plt.xlabel('Token Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Early stopping parameters\n",
    "best_val_loss = np.inf\n",
    "patience = 3  # Number of epochs to tolerate no improvement\n",
    "patience_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"BERT Stance Detection\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", 2e-5)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", train_dataloader.batch_size)\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # BERT requires a special optimizer and a learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 10\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Function to calculate the accuracy of our predictions vs labels\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    # Function for formatting elapsed times\n",
    "    def format_time(elapsed):\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "    # Training Loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch_i in range(0, epochs):\n",
    "        \n",
    "        # Training\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)            \n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\n",
    "        # Validation\n",
    "        print(\"\\nRunning Validation...\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():        \n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Calculate precision, recall, and F1-score\n",
    "        predictions, true_labels = get_predictions(model, validation_dataloader) # Assuming get_predictions function\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        report = classification_report(true_labels, predicted_labels, output_dict=True)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch_i)\n",
    "\n",
    "        # Log precision, recall, and F1-score for each class\n",
    "        for label, metrics in report.items():\n",
    "            if label not in [\"accuracy\", \"macro avg\", \"weighted avg\"]:\n",
    "                mlflow.log_metric(f\"class_{label}_precision\", metrics[\"precision\"], step=epoch_i)\n",
    "                mlflow.log_metric(f\"class_{label}_recall\", metrics[\"recall\"], step=epoch_i)\n",
    "                mlflow.log_metric(f\"class_{label}_f1-score\", metrics[\"f1-score\"], step=epoch_i)\n",
    "\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "        mlflow.log_metric(\"avg_train_loss\", avg_train_loss, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "        mlflow.log_metric(\"avg_val_loss\", avg_val_loss, step=epoch_i)\n",
    "        # Log macro and weighted average of precision, recall, and F1-score\n",
    "        mlflow.log_metric(\"macro_avg_precision\", report[\"macro avg\"][\"precision\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"macro_avg_recall\", report[\"macro avg\"][\"recall\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"macro_avg_f1-score\", report[\"macro avg\"][\"f1-score\"], step=epoch_i)\n",
    "\n",
    "        mlflow.log_metric(\"weighted_avg_precision\", report[\"weighted avg\"][\"precision\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"weighted_avg_recall\", report[\"weighted avg\"][\"recall\"], step=epoch_i)\n",
    "        mlflow.log_metric(\"weighted_avg_f1-score\", report[\"weighted avg\"][\"f1-score\"], step=epoch_i)\n",
    "\n",
    "        # Check for improvement\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0  # reset counter\n",
    "            print(f\"New best model found at epoch {epoch_i+1}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            mlflow.pytorch.log_model(model, \"model\", registered_model_name=\"BERT-Stance-Detection-Best\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in epoch {epoch_i+1}. Early stopping counter: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "patience = 3\n",
    "patience_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "mlflow.set_experiment(\"BERT Stance Detection\")\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"learning_rate\", 2e-5)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", train_dataloader.batch_size)\n",
    "\n",
    "    # Set the seed value all over the place to make this reproducible\n",
    "    seed_val = 42\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # BERT requires a special optimizer and a learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    epochs = 10\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    # Function to calculate the accuracy of our predictions vs labels\n",
    "    def flat_accuracy(preds, labels):\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    # Function for formatting elapsed times\n",
    "    def format_time(elapsed):\n",
    "        elapsed_rounded = int(round((elapsed)))\n",
    "        return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "    # Training Loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch_i in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()        \n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch_i)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        total_eval_accuracy = 0\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = outputs.logits\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "        mlflow.log_metric(\"val_loss\", avg_val_loss, step=epoch_i)\n",
    "        mlflow.log_metric(\"val_accuracy\", avg_val_accuracy, step=epoch_i)\n",
    "\n",
    "        # Check if this is the best model based on validation loss\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0  # Reset patience counter\n",
    "            print(f\"New best model found at epoch {epoch_i+1}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {patience} epochs with no improvement\")\n",
    "                break\n",
    "\n",
    "    # Save the best model to MLflow\n",
    "    mlflow.pytorch.log_model(model, \"model\", registered_model_name=\"Best BERT Model\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning, module=\".*tensorflow.*\")\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow_data_validation as tfdv\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from zenml.steps import step\n",
    "from zenml.pipelines import pipeline\n",
    "import tweepy\n",
    "import os\n",
    "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'\n",
    "import json\n",
    "import requests\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_data_validation as tfdv\n",
    "import pandas as pd\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import BertTokenizer\n",
    "from cassandra.cluster import Cluster\n",
    "from cassandra.query import BatchStatement\n",
    "from cassandra import ConsistencyLevel\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import logging\n",
    "from cassandra.policies import DCAwareRoundRobinPolicy\n",
    "import pytest\n",
    "\n",
    "#Final pipeline until training\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Constants and Hyperparameters\n",
    "CSV_FILE_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5382_SP24_FINALPROJECT\\\\scripts\\\\dataset_reduced.csv'\n",
    "PRETRAINED_LM_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden'\n",
    "HYPERPARAMS = {\n",
    "    \"batch_size\": 4,             # Batch size for training\n",
    "    \"learning_rate\": 1e-5,       # Learning rate for the optimizer\n",
    "    \"epochs\": 10,                # Number of training epochs\n",
    "    \"max_length\": 150,           # Max length for tokenized sequences\n",
    "    \"num_labels\": 3,             # Number of output labels for classification\n",
    "    \"ignore_mismatched_sizes\": True,  # To ignore size mismatches in the model\n",
    "    \"optimizer_eps\": 1e-8,       # Epsilon value for the AdamW optimizer\n",
    "    \"num_warmup_steps\": 0,       # Number of warmup steps for the schedulerHYPERPARAMS = {\n",
    "}\n",
    "print(\"Hyperparameters:\", HYPERPARAMS)\n",
    "mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_LM_PATH)\n",
    "\n",
    "\n",
    "@step\n",
    "def load_data(csv_file_path):\n",
    "    \"\"\"\n",
    "    Step to load data from a CSV file. This function includes error handling,\n",
    "    logging, and a greeting message for a user-friendly interface. The approach\n",
    "    is inspired by the meticulous data handling seen in the stance detection paper.\n",
    "\n",
    "    Args:\n",
    "    csv_file_path (str): Path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: Loaded data or an informative error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Greetings! Initiating the data loading process.\")\n",
    "        # Load the dataset\n",
    "        data = pd.read_csv(csv_file_path)\n",
    "        #Reset index\n",
    "        data.set_index('tweet_id', inplace = True)\n",
    "        # Log successful loading\n",
    "        logging.info(f\"Data successfully loaded from {csv_file_path}.\")\n",
    "        #Preview\n",
    "        data.head()\n",
    "        print(f\"Data from {csv_file_path} loaded successfully. The dataset contains {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "\n",
    "        # Perform initial checks similar to the paper's approach, e.g., checking for null values\n",
    "        if data.isnull().values.any():\n",
    "            null_counts = data.isnull().sum()\n",
    "            print(\"Warning: Null values found in the dataset.\")\n",
    "            print(f\"Null value counts by column:\\n{null_counts[null_counts > 0]}\")\n",
    "\n",
    "        # Return the loaded data\n",
    "        return data\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"File not found: {csv_file_path}\")\n",
    "        print(f\"Error: The file {csv_file_path} was not found. Please check the file path.\")\n",
    "\n",
    "    except pd.errors.ParserError:\n",
    "        logging.error(f\"Error parsing the file: {csv_file_path}\")\n",
    "        print(f\"Error: Could not parse the file {csv_file_path}. Please check if the file is a valid CSV.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "@step\n",
    "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step for data preprocessing, including cleaning, tokenization, and label encoding.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The loaded data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Preprocessed data.\n",
    "    \"\"\"\n",
    "    def clean_text(text):\n",
    "        \"\"\"\n",
    "        Function to clean text data.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        # Removing URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        # Removing usernames and hashtags\n",
    "        text = re.sub(r'@\\S+|#\\S+', '', text)\n",
    "        # Removing special characters and numbers\n",
    "        text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "        # Converting to lowercase\n",
    "        text = text.lower().strip()\n",
    "        # Tokenize and rejoin the text to ensure clean tokenization\n",
    "        tokens = text.split()\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    # Ensure the dataframe has the expected columns\n",
    "    expected_columns = {'text','label'}\n",
    "    data.info()\n",
    "    data.isnull().sum()\n",
    "    if not expected_columns.issubset(data.columns):\n",
    "        missing_cols = expected_columns - set(data.columns)\n",
    "        error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "    # Clean text data\n",
    "    #data['text'] = data['text'].astype(str).apply(clean_text)\n",
    "\n",
    "    print(\"Preprocessing the data...\")\n",
    "    data.text.iloc[10]\n",
    "    data.label.value_counts()\n",
    "    missing_label_rows = data[data['label'].isna()]\n",
    "    data.dropna(subset=['label'], inplace=True)\n",
    "\n",
    "    if not missing_label_rows.empty:\n",
    "        print(f\"Rows with missing labels:\\n{missing_label_rows}\")\n",
    "    # If the DataFrame is empty after dropping missing values, return it as is\n",
    "    if data.empty:\n",
    "        print(\"The DataFrame is empty after preprocessing.\")\n",
    "        return data\n",
    "    # Encode labels\n",
    "    label_mapping = {'NONE': 0, 'FAVOR': 1, 'AGAINST': 2}\n",
    "    data['label'] = data['label'].map(label_mapping)\n",
    "\n",
    "    # Validate the encoding\n",
    "    unique_labels = data['label'].unique()\n",
    "    if set(unique_labels) != {0, 1, 2}:\n",
    "        error_msg = f\"Labels are not correctly mapped. Found unique labels: {unique_labels}\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "    print(\"Data preprocessing complete. Text has been cleaned and labels encoded.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@step\n",
    "def visualize_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step for data visualization including checking for missing values and data distribution.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The preprocessed data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data (if the next step in the pipeline requires it).\n",
    "    \"\"\"\n",
    "    print(\"Starting data visualization process...\")\n",
    "\n",
    "    try:\n",
    "        # Visualize Missing Values\n",
    "        missing_values = data.isnull().sum()\n",
    "        missing_percentage = (missing_values / len(data)) * 100\n",
    "        missing_df = pd.DataFrame({'Feature': missing_values.index, 'MissingValues': missing_values, 'Percentage': missing_percentage})\n",
    "\n",
    "        # Filter out features with no missing values\n",
    "        missing_df = missing_df[missing_df['MissingValues'] > 0].sort_values('Percentage', ascending=False)\n",
    "\n",
    "        # Plotting missing values (if there are any)\n",
    "        if not missing_df.empty:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='Percentage', y='Feature', data=missing_df)\n",
    "            plt.title('Percentage of Missing Values per Feature')\n",
    "            plt.xlabel('Percentage')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No missing values found in the dataset.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred during missing values visualization: {e}\")\n",
    "        print(f\"Error occurred during missing values visualization: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Data Distribution Visualization\n",
    "        if 'label' in data.columns:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.countplot(x='label', data=data)\n",
    "            plt.title('Distribution of Labels')\n",
    "            plt.xlabel('Label')\n",
    "            plt.ylabel('Count')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Label column not found, skipping label distribution visualization.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error occurred during label distribution visualization: {e}\")\n",
    "        print(f\"Error occurred during label distribution visualization: {e}\")\n",
    "\n",
    "    print(\"Data visualization process completed.\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@step\n",
    "def split_data_1(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step to split the data into training, validation, and testing sets, but only return the training set.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the preprocessed data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The training data set.\n",
    "    \"\"\"\n",
    "    # Define the size for your test and validation sets\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.1\n",
    "\n",
    "    # Initial split to separate out the test set\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Calculate the adjusted validation size based on the remaining data after test split\n",
    "    adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "    # Split the remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)\n",
    "\n",
    "    # Return only the training data\n",
    "    return train_data\n",
    "\n",
    "\n",
    "@step\n",
    "def split_data_2(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step to split the data into training, validation, and testing sets, but only return the validation set.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the preprocessed data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The validation data set.\n",
    "    \"\"\"\n",
    "    # Define the size for your test and validation sets\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.1\n",
    "\n",
    "    # Initial split to separate out the test set\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Calculate the adjusted validation size based on the remaining data after test split\n",
    "    adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "    # Split the remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)\n",
    "\n",
    "    # Return only the validation data\n",
    "    return val_data\n",
    "\n",
    "\n",
    "@step\n",
    "def split_data_3(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step to split the data into training, validation, and testing sets, but only return the testing set.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the preprocessed data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The testing data set.\n",
    "    \"\"\"\n",
    "    # Define the size for your test and validation sets\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.1\n",
    "\n",
    "    # Initial split to separate out the test set\n",
    "    train_val_data, test_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Calculate the adjusted validation size based on the remaining data after test split\n",
    "    adjusted_validation_size = validation_size / (1 - test_size)\n",
    "\n",
    "    # Split the remaining data into training and validation sets\n",
    "    train_data, val_data = train_test_split(train_val_data, test_size=adjusted_validation_size, random_state=42)\n",
    "\n",
    "    # Return only the test data\n",
    "    return test_data\n",
    "\n",
    "\n",
    "@step\n",
    "def validate_data(train_data: pd.DataFrame, val_data: pd.DataFrame, test_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step to validate data using TensorFlow Data Validation (TFDV).\n",
    "\n",
    "    Args:\n",
    "        train_data (pandas.DataFrame): The DataFrame containing the training data.\n",
    "        val_data (pandas.DataFrame): The DataFrame containing the validation data.\n",
    "        test_data (pandas.DataFrame): The DataFrame containing the test data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The revised test_data DataFrame after handling anomalies, if found.\n",
    "    \"\"\"\n",
    "    # Generate statistics for training, validation, and test data\n",
    "    try:\n",
    "        train_stats = tfdv.generate_statistics_from_dataframe(train_data)\n",
    "        val_stats = tfdv.generate_statistics_from_dataframe(val_data)\n",
    "        test_stats = tfdv.generate_statistics_from_dataframe(test_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in statistics generation: {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(\"Statistics generated for Train, Validation, and Test datasets.\")\n",
    "\n",
    "    # Infer schema from training data\n",
    "    schema = tfdv.infer_schema(statistics=test_stats)\n",
    "    print(\"Schema inferred from the testing data.\")\n",
    "\n",
    "    # Validate validation and test data using the inferred schema\n",
    "    try:\n",
    "        for dataset_name, dataset_stats in [('Validation', val_stats), ('Test', test_stats)]:\n",
    "            print(f\"Validating {dataset_name} data...\")\n",
    "            anomalies = tfdv.validate_statistics(statistics=dataset_stats, schema=schema)\n",
    "            \n",
    "            if anomalies.anomaly_info:\n",
    "                print(f\"Anomalies found in {dataset_name} data:\")\n",
    "                tfdv.display_anomalies(anomalies)\n",
    "                # Potential handling or correction of anomalies\n",
    "                # E.g., aligning categorical features, handling missing values, etc.\n",
    "                # This will depend on the nature of anomalies detected\n",
    "            else:\n",
    "                print(f\"No anomalies detected in {dataset_name} data.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data validation: {e}\")\n",
    "        raise e\n",
    "\n",
    "    # Display schema for review\n",
    "    print(\"Displaying the inferred schema:\")\n",
    "    tfdv.display_schema(schema=schema)\n",
    "\n",
    "    return test_data\n",
    "\n",
    "import random\n",
    "\n",
    "@step\n",
    "def feature_engineering(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step for feature engineering using BERT tokenizer and storing features in Cassandra.\n",
    "    Adds error handling, logging, and user-friendly print statements.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Input DataFrame with text data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with engineered features.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Connecting to the Cassandra Database...\")\n",
    "        cluster = Cluster(contact_points=['127.0.0.1'],port=9042,load_balancing_policy=DCAwareRoundRobinPolicy())\n",
    "        session = cluster.connect()\n",
    "        session.set_keyspace('keyspace')\n",
    "        session.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS \"keyspace\".features (\n",
    "                id UUID PRIMARY KEY,\n",
    "                features list<int>\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        insert_statement = session.prepare('INSERT INTO \"keyspace\".features (id, features) VALUES (?, ?)')\n",
    "        batch = BatchStatement(consistency_level=ConsistencyLevel.ONE)\n",
    "        batch_size_limit = 8\n",
    "        if 'tokens' not in data.columns:\n",
    "            data['tokens'] = None\n",
    "\n",
    "        for index, row in data.iterrows():\n",
    "            row_id = uuid.uuid4()\n",
    "            tokens = tokenizer.encode(row['text'], add_special_tokens=True)\n",
    "            data.at[index, 'tokens'] = tokens\n",
    "            batch.add(insert_statement, (row_id, tokens))\n",
    "\n",
    "            if len(batch) >= batch_size_limit:\n",
    "                session.execute(batch)\n",
    "                batch.clear()\n",
    "\n",
    "        if len(batch) > 0:\n",
    "            session.execute(batch)\n",
    "\n",
    "        print(\"Feature engineering and data insertion completed successfully.\")\n",
    "        return data\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during feature engineering: {e}\")\n",
    "        print(f\"An error occurred during feature engineering: {e}\")\n",
    "        # Optionally, re-raise the exception if you want the step to fail in case of errors\n",
    "        raise e\n",
    "\n",
    "    finally:\n",
    "        # Ensure the Cassandra connection is always closed\n",
    "        session.shutdown()\n",
    "        cluster.shutdown()\n",
    "def compute_confusion_matrix(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes the confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "    true_labels (array): Array of true labels.\n",
    "    predictions (array): Array of model predictions.\n",
    "\n",
    "    Returns:\n",
    "    ndarray: Confusion matrix.\n",
    "    \"\"\"\n",
    "    return confusion_matrix(true_labels, predictions)\n",
    "\n",
    "def compute_accuracy(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes the accuracy score.\n",
    "    \n",
    "    Args:\n",
    "    true_labels (array): Array of true labels.\n",
    "    predictions (array): Array of model predictions.\n",
    "\n",
    "    Returns:\n",
    "    float: Accuracy score.\n",
    "    \"\"\"\n",
    "    return accuracy_score(true_labels, predictions)\n",
    "\n",
    "def compute_recall(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes the recall score.\n",
    "    \n",
    "    Args:\n",
    "    true_labels (array): Array of true labels.\n",
    "    predictions (array): Array of model predictions.\n",
    "\n",
    "    Returns:\n",
    "    float: Recall score.\n",
    "    \"\"\"\n",
    "    return recall_score(true_labels, predictions, average='macro')\n",
    "\n",
    "def compute_precision(true_labels, predictions):\n",
    "    \"\"\"\n",
    "    Computes the precision score.\n",
    "    \n",
    "    Args:\n",
    "    true_labels (array): Array of true labels.\n",
    "    predictions (array): Array of model predictions.\n",
    "\n",
    "    Returns:\n",
    "    float: Precision score.\n",
    "    \"\"\"\n",
    "    return precision_score(true_labels, predictions, average='macro')\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    val_labels = []\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            for batch in data_loader:\n",
    "                # Ensure all tensors are on the same device\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(torch.argmax(logits, axis=1).cpu().numpy())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during evaluation: {e}\")\n",
    "            return None\n",
    "\n",
    "    # Compute detailed metrics\n",
    "    report = classification_report(val_labels, val_preds, output_dict=True)\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "    cm = compute_confusion_matrix(val_labels, val_preds)\n",
    "    accuracy = compute_accuracy(val_labels, val_preds)\n",
    "    recall = compute_recall(val_labels, val_preds)\n",
    "    precision = compute_precision(val_labels, val_preds)\n",
    "\n",
    "    # Include metrics for each class if necessary\n",
    "    detailed_metrics = {f'class_{k}': v for k, v in report.items() if k.isdigit()}\n",
    "\n",
    "    return {\n",
    "        'confusion_matrix': cm, \n",
    "        'accuracy': accuracy, \n",
    "        'precision': precision, \n",
    "        'recall': recall, \n",
    "        'f1': f1,\n",
    "        'detailed_metrics': detailed_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "@step\n",
    "def train_model(data: pd.DataFrame,train_data: pd.DataFrame, val_data: pd.DataFrame, hyperparams: dict) -> BertForSequenceClassification:\n",
    "    \"\"\"\n",
    "    Step to train the BERT model using PyTorch, with MLflow tracking.\n",
    "    Includes error handling, logging, and detailed progress updates.\n",
    "\n",
    "    Args:\n",
    "        data\n",
    "        train_data (pd.DataFrame): Training data.\n",
    "        val_data (pd.DataFrame): Validation data.\n",
    "        hyperparams (dict): Hyperparameters for training.\n",
    "\n",
    "    Returns:\n",
    "        BertForSequenceClassification: Trained BERT model.\n",
    "    \"\"\"\n",
    "    PRETRAINED_LM_PATH = 'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\SP24\\\\bert-election2020-twitter-stance-biden'\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Use a temporary directory for checkpoints\n",
    "    checkpoint_dir = os.path.join(os.getcwd(), \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\SP24\\\\scripts\\\\model_checkpoints\")\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    def check_labels(data):\n",
    "        unique_labels = np.unique(data['label'])\n",
    "        if not np.array_equal(unique_labels, [0, 1, 2]):\n",
    "            raise ValueError(f\"Labels out of bounds. Expected labels [0, 1, 2], found {unique_labels}\")\n",
    "    expected_columns = {'text','label'}\n",
    "    if not expected_columns.issubset(data.columns):\n",
    "        missing_cols = expected_columns - set(data.columns)\n",
    "        error_msg = f\"The dataframe is missing the following required columns: {', '.join(missing_cols)}\"\n",
    "        logging.error(error_msg)\n",
    "        raise ValueError(error_msg)\n",
    "    missing_label_rows = data[data['label'].isna()]\n",
    "    data.dropna(subset=['label'], inplace=True)\n",
    "    if not missing_label_rows.empty:\n",
    "        print(f\"Rows with missing labels:\\n{missing_label_rows}\")\n",
    "    # If the DataFrame is empty after dropping missing values, return it as is\n",
    "    if data.empty:\n",
    "        print(\"The DataFrame is empty after preprocessing.\")\n",
    "    # Encode labels\n",
    "    label_mapping = {'NONE': 0, 'FAVOR': 1, 'AGAINST': 2}\n",
    "    data['label'] = data['label'].map(label_mapping)\n",
    "    data.head(15)\n",
    "    data['data_type'] = ['not_set'] * data.shape[0]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(data.index.values, data.label.values,test_size = 0.15,random_state = 17,stratify = data.label.values)\n",
    "    data.loc[X_train, 'data_type'] = 'train'\n",
    "    data.loc[X_val, 'data_type'] = 'val'\n",
    "    data.groupby(['label', 'data_type']).count()\n",
    "    #Tokenize train set\n",
    "    encoded_data_train = tokenizer.batch_encode_plus(data[data.data_type == 'train'].text.values,\n",
    "                                                add_special_tokens = True,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 150,\n",
    "                                                return_tensors = 'pt')\n",
    "    #Tokenizer val set\n",
    "    encoded_data_val = tokenizer.batch_encode_plus(data[data.data_type == 'val'].text.values,\n",
    "                                                #add_special_tokens = True,\n",
    "                                                return_attention_mask = True,\n",
    "                                                pad_to_max_length = True,\n",
    "                                                max_length = 150,\n",
    "                                                return_tensors = 'pt')\n",
    "    encoded_data_train\n",
    "    #Encode train set\n",
    "    input_ids_train = encoded_data_train['input_ids']\n",
    "    attention_masks_train = encoded_data_train['attention_mask']\n",
    "    labels_train = torch.tensor(data[data.data_type == 'train'].label.values)\n",
    "    #Encode val set\n",
    "    input_ids_val = encoded_data_val['input_ids']\n",
    "    attention_masks_val = encoded_data_val['attention_mask']\n",
    "\n",
    "    #Convert data type to torch.tensor\n",
    "    labels_val = torch.tensor(data[data.data_type == 'val'].label.values)\n",
    "\n",
    "    #Create dataloader\n",
    "    dataset_train = TensorDataset(input_ids_train, attention_masks_train,labels_train)\n",
    "\n",
    "    dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "    print(len(dataset_train))\n",
    "    print(len(dataset_val))\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM_PATH, num_labels=3, ignore_mismatched_sizes=True)\n",
    "    model.config\n",
    "\n",
    "    from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "    batch_size = 4 #Since we have limited resource\n",
    "\n",
    "    #Load train set\n",
    "    dataloader_train = DataLoader(dataset_train,sampler = RandomSampler(dataset_train),batch_size = batch_size)\n",
    "\n",
    "    #Load val set\n",
    "    dataloader_val = DataLoader(dataset_val,sampler = RandomSampler(dataset_val),batch_size = 32) #since we don't have to do backpropagation for this step\n",
    "\n",
    "    from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "    epochs = 10\n",
    "\n",
    "    #Load optimizer\n",
    "    optimizer = AdamW(model.parameters(),lr = 1e-5,eps = 1e-8) #2e-5 > 5e-5\n",
    "\n",
    "    #Load scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps = 0,num_training_steps = len(dataloader_train)*epochs)\n",
    "\n",
    "    #F1 score\n",
    "    def f1_score_func(preds, labels):\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        return f1_score(labels_flat, preds_flat, average = 'weighted')\n",
    "    \n",
    "    #Accuracy score\n",
    "    def accuracy_per_class(preds, labels):\n",
    "        label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "        \n",
    "        #Make prediction\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "        labels_flat = labels.flatten()\n",
    "        \n",
    "        for label in np.unique(labels_flat):\n",
    "            y_preds = preds_flat[labels_flat==label]\n",
    "            y_true = labels_flat[labels_flat==label]\n",
    "            print(f'Class: {label_dict_inverse[label]}')\n",
    "            print(f'Accuracy:{len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n",
    "\n",
    "    def evaluate(dataloader_val):\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #Evaluation mode disables the dropout layer \n",
    "        model.eval()\n",
    "        \n",
    "        #Tracking variables\n",
    "        loss_val_total = 0\n",
    "        predictions, true_vals = [], []\n",
    "        \n",
    "        for batch in tqdm(dataloader_val):\n",
    "            \n",
    "            #Load into GPU\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            \n",
    "            #Define inputs\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                    'attention_mask': batch[1],\n",
    "                    'labels':         batch[2]}\n",
    "\n",
    "            #Compute logits\n",
    "            with torch.no_grad():        \n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            #Compute loss\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            loss_val_total += loss.item()\n",
    "\n",
    "            #Compute accuracy\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = inputs['labels'].cpu().numpy()\n",
    "            predictions.append(logits)\n",
    "            true_vals.append(label_ids)\n",
    "            \n",
    "        #Compute average loss\n",
    "        loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        true_vals = np.concatenate(true_vals, axis=0)\n",
    "        return loss_val_avg, predictions, true_vals\n",
    "\n",
    "    seed_val = 17\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    try:\n",
    "        with mlflow.start_run():\n",
    "            print(\"Initializing the training process...\")\n",
    "            check_labels(train_data)\n",
    "            check_labels(val_data)\n",
    "            mlflow.log_param(\"batch_size\", batch_size)\n",
    "            mlflow.log_param(\"learning_rate\", 1e-5)\n",
    "            mlflow.log_param(\"epochs\", epochs)\n",
    "\n",
    "            model = BertForSequenceClassification.from_pretrained(PRETRAINED_LM_PATH, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "            # Progress bar for epochs\n",
    "            epoch_progress = tqdm(range(1, epochs+1), desc=\"Training Progress\", leave=True)\n",
    "            # Calculate class weights for handling class imbalance\n",
    "            class_weights = compute_class_weight(class_weight='balanced', \n",
    "                                                classes=np.unique(train_data['label']), \n",
    "                                                y=train_data['label'])\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "            # Initialize the optimizer with a lower learning rate\n",
    "            optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=len(dataloader_train)*epochs)\n",
    "\n",
    "            # Implement early stopping\n",
    "            best_val_loss = float('inf')\n",
    "            no_improvement_epochs = 0\n",
    "            early_stopping_threshold = 3  # stop training if no improvement for 3 epochs\n",
    "\n",
    "            for epoch in epoch_progress:\n",
    "                model.train()\n",
    "                loss_train_total = 0\n",
    "\n",
    "                # Nested progress bar for training batches\n",
    "                batch_progress = tqdm(dataloader_train, desc=f'Epoch {epoch}/{epochs}', leave=False)\n",
    "                for batch in batch_progress:\n",
    "                    model.zero_grad()\n",
    "                    batch = tuple(b.to(device) for b in batch)\n",
    "                    inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "                    outputs = model(**inputs)\n",
    "                    loss = outputs[0]\n",
    "                    loss_train_total += loss.item()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    # Update progress bar for batches\n",
    "                    batch_progress.set_postfix({'Batch Loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "                # Calculate and display epoch metrics\n",
    "                loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "                epoch_progress.set_postfix({'Epoch Avg Loss': '{:.3f}'.format(loss_train_avg)})\n",
    "\n",
    "                val_loss, predictions, true_vals = evaluate(dataloader_val)\n",
    "                val_f1 = f1_score_func(predictions, true_vals)\n",
    "\n",
    "                # Log metrics in tqdm and MLflow\n",
    "                tqdm.write(f'Epoch {epoch}/{epochs} - Training Loss: {loss_train_avg}, Validation Loss: {val_loss}, F1 Score: {val_f1}')\n",
    "                mlflow.log_metric(\"training_loss\", loss_train_avg, step=epoch)\n",
    "                mlflow.log_metric(\"validation_loss\", val_loss, step=epoch)\n",
    "                mlflow.log_metric(\"F1_score\", val_f1, step=epoch)\n",
    "                # Saving the model checkpoint after each epoch\n",
    "                checkpoint_dir = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\SP24\\\\scripts\\\\model_checkpoints\"\n",
    "                epoch_model_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch}.pt\")\n",
    "                torch.save(model.state_dict(), epoch_model_path)\n",
    "                # Log the checkpoint in MLflow\n",
    "                mlflow.log_artifact(epoch_model_path)\n",
    "                # Early stopping check\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    no_improvement_epochs = 0\n",
    "                else:\n",
    "                    no_improvement_epochs += 1\n",
    "                    if no_improvement_epochs >= early_stopping_threshold:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "                        torch.save(model.state_dict(), epoch_model_path)\n",
    "                        mlflow.log_artifact(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\SP24\\\\scripts\\\\model_checkpoints\")\n",
    "                        break\n",
    "                torch.save(model.state_dict(), epoch_model_path)\n",
    "                mlflow.log_artifact(\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\SP24\\\\scripts\\\\model_checkpoints\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during model training: {e}\")\n",
    "        print(f\"An error occurred during model training: {e}\")\n",
    "        mlflow.end_run()\n",
    "        raise e\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def test_model(test_data: pd.DataFrame, model: BertForSequenceClassification, tokenizer: BertTokenizer, device: torch.device) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the trained model using the test dataset and logs the performance metrics using MLflow.\n",
    "\n",
    "    Args:\n",
    "        test_data (pd.DataFrame): Test data.\n",
    "        model (BertForSequenceClassification): Trained BERT model.\n",
    "        tokenizer (BertTokenizer): Tokenizer used for data preparation.\n",
    "        device (torch.device): Device to run the model on (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        None: Directly logs the evaluation metrics to MLflow.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_dataloader = create_dataloader(test_data, tokenizer)  # Function to create a DataLoader for the test set\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {'input_ids': batch[0].to(device), 'attention_mask': batch[1].to(device), 'labels': batch[2].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)\n",
    "            all_labels.extend(batch[2].numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric('Accuracy', accuracy)\n",
    "    mlflow.log_metric('Precision', precision)\n",
    "    mlflow.log_metric('Recall', recall)\n",
    "    mlflow.log_metric('F1 Score', f1)\n",
    "\n",
    "    # Check if the test is successful\n",
    "    if f1 > 0.8 and accuracy > 0.8:\n",
    "        print(\"Test successful: High model performance.\")\n",
    "    else:\n",
    "        print(\"Test failed: Model performance below expected thresholds.\")\n",
    "\n",
    "def create_dataloader(data: pd.DataFrame, tokenizer: BertTokenizer) -> DataLoader:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame into a DataLoader for evaluation.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Data to be evaluated.\n",
    "        tokenizer (BertTokenizer): Tokenizer to process the text data.\n",
    "\n",
    "    Returns:\n",
    "        DataLoader: DataLoader for the model evaluation.\n",
    "    \"\"\"\n",
    "    # Similar tokenization and DataLoader creation as done in training\n",
    "    pass  # Implement according to your existing data preparation logic\n",
    "\n",
    "# Add the evaluate_model step after the train_model step in the pipeline\n",
    "model = train_model_step(data, train_data, val_data, HYPERPARAMS)\n",
    "test_data = split_data_3_step(data=preprocessed_data)\n",
    "test_model(test_data, model, tokenizer, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert: accuracy dropped below threshold. Value: 0.7272727272727273, Threshold: 0.91\n",
      "Metric precision is within the threshold\n",
      "Alert: recall dropped below threshold. Value: 0.7272727272727273, Threshold: 0.91\n",
      "Metric f1 is within the threshold\n"
     ]
    }
   ],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Define thresholds for various metrics that indicate acceptable model performance.\n",
    "THRESHOLDS = {\n",
    "    \"accuracy\": 0.91,   # Minimum acceptable accuracy\n",
    "    \"precision\": 0.70, # Minimum acceptable precision\n",
    "    \"recall\": 0.91,     # Minimum acceptable recall\n",
    "    \"f1\": 0.70         # Minimum acceptable f1-score\n",
    "}\n",
    "\n",
    "def latest_metrics(experiment_name):\n",
    "    \"\"\"\n",
    "    Fetches the latest metrics from MLflow for the given experiment name.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "        dict: Latest metrics if available, otherwise error message as a string.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    if not experiment:\n",
    "        return \"Experiment not found.\"\n",
    "\n",
    "    runs = client.search_runs(experiment_ids=[experiment.experiment_id], order_by=[\"start_time DESC\"], max_results=1)\n",
    "    if not runs:\n",
    "        return \"No runs found.\"\n",
    "\n",
    "    return runs[0].data.metrics\n",
    "\n",
    "def metrics_vs_thresholds(metrics):\n",
    "    \"\"\"\n",
    "    Checks the fetched metrics against predefined thresholds and generates alerts if any metrics are below thresholds.\n",
    "\n",
    "    Args:\n",
    "        metrics (dict): Metrics fetched from MLflow.\n",
    "\n",
    "    Returns:\n",
    "        list: Alerts as a list of strings.\n",
    "    \"\"\"\n",
    "    if isinstance(metrics, str):\n",
    "        return [metrics]  # Return the error message as a list\n",
    "    alerts = []\n",
    "    for metric, threshold in THRESHOLDS.items():\n",
    "        if metric in metrics and metrics[metric] < threshold:\n",
    "            alerts.append(f\"Alert: {metric} dropped below threshold. Value: {metrics[metric]}, Threshold: {threshold}\")\n",
    "        else:\n",
    "            alerts.append(f\"Metric {metric} is within the threshold\")\n",
    "    return alerts\n",
    "\n",
    "# Example usage\n",
    "experiment_name = \"experiment_bert-election2020-twitter-stance-biden\"  # Replace with your actual experiment name\n",
    "metrics = latest_metrics(experiment_name)\n",
    "alerts = metrics_vs_thresholds(metrics)\n",
    "for alert in alerts:\n",
    "    print(alert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Example hyperparameter space\n",
    "hyperparameter_space = {\n",
    "    'learning_rate': uniform(1e-5, 5e-5),\n",
    "    'batch_size': randint(16, 64),\n",
    "    'epochs': randint(2, 5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set the tracking URI if necessary\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000/\")\n",
    "\n",
    "# Get a list of all registered models\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "for rm in client.list_registered_models():\n",
    "    print(f\"Registered Model Name: {rm.name}\")\n",
    "    for version in rm.latest_versions:\n",
    "        print(f\"Version: {version.version}, Stage: {version.current_stage}, URI: {version.source}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from zenml.steps import step\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from fastapi import FastAPI\n",
    "\n",
    "\n",
    "@step\n",
    "def setup_fastapi_step(model_name: str) -> FastAPI:\n",
    "    \"\"\"\n",
    "    Step to configure a FastAPI app with a pre-trained BERT model for sequence classification.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Path to the BERT model directory.\n",
    "\n",
    "    Returns:\n",
    "        FastAPI: Configured FastAPI application.\n",
    "    \"\"\"\n",
    "    from fastapi import FastAPI, HTTPException\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    # Initialize FastAPI app\n",
    "    app = FastAPI()\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Define Pydantic model for input validation\n",
    "    class InputText(BaseModel):\n",
    "        text: str\n",
    "\n",
    "    # Define prediction endpoint\n",
    "    @app.post(\"/predict\")\n",
    "    async def predict(input_text: InputText):\n",
    "        if not input_text.text:\n",
    "            raise HTTPException(status_code=400, detail=\"Text field cannot be empty\")\n",
    "        inputs = tokenizer(input_text.text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_index = torch.argmax(logits, dim=1).item()\n",
    "            return {\"predicted_class\": class_names[predicted_class_index]}  # Assuming class_names is defined\n",
    "\n",
    "    return app\n",
    "\n",
    "# Note: Ensure all dependencies (FastAPI, transformers, pydantic, torch) are installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_path = \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "class TextData(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data: TextData):\n",
    "    inputs = tokenizer(data.text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_index = torch.argmax(logits, dim=1).item()\n",
    "    return {\"predicted_class\": predicted_class_index}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, File, UploadFile, Request, Form\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "import uvicorn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import logging\n",
    "from functools import lru_cache\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "app = FastAPI()\n",
    "\n",
    "# Assuming static files are also in the scripts directory or a subdirectory therein\n",
    "app.mount(\"/scripts\", StaticFiles(directory=\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5356_SP24\\\\scripts\"), name=\"scripts\")\n",
    "\n",
    "\n",
    "# Setup for templates\n",
    "templates = Jinja2Templates(directory=\"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\CSC5356_SP24\\\\scripts\")\n",
    "\n",
    "class Config:\n",
    "    MODEL_PATH = os.getenv(\"MODEL_PATH\", \"C:\\\\Users\\\\LENOVO\\\\Desktop\\\\bert-election2020-twitter-stance-biden\")\n",
    "\n",
    "@lru_cache()\n",
    "def load_model():\n",
    "    logging.info(\"Loading the BERT model from path: %s\", Config.MODEL_PATH)\n",
    "    tokenizer = BertTokenizer.from_pretrained(Config.MODEL_PATH)\n",
    "    model = BertForSequenceClassification.from_pretrained(Config.MODEL_PATH)\n",
    "    return tokenizer, model\n",
    "\n",
    "@app.get(\"/predict_form/\")\n",
    "async def predict_form(request: Request):\n",
    "    return templates.TemplateResponse(\"prediction_form.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/predict/\")\n",
    "async def predict(text: str = Form(...)):\n",
    "    label_descriptions = {\n",
    "        0: \"Negative\",\n",
    "        1: \"Positive\",\n",
    "        2: \"Neutral\"\n",
    "    }\n",
    "    tokenizer, model = load_model()\n",
    "    try:\n",
    "        encoded_input = tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=128,\n",
    "            return_attention_mask=True, return_tensors='pt', truncation=True\n",
    "        )\n",
    "        input_ids = encoded_input['input_ids']\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_id = logits.argmax().item()\n",
    "\n",
    "        description = label_descriptions.get(predicted_class_id, \"Unknown\")\n",
    "\n",
    "        return {\"text\": text, \"class_id\": predicted_class_id, \"description\": description}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process prediction: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n",
    "\n",
    "\n",
    "@app.get(\"/upload_form/\")\n",
    "async def upload_form(request: Request):\n",
    "    return templates.TemplateResponse(\"upload_form.html\", {\"request\": request})\n",
    "\n",
    "@app.post(\"/load_data/\")\n",
    "async def load_data(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        df = pd.read_csv(file.file)\n",
    "        return {\"message\": \"Data loaded successfully\", \"data\": df.head().to_dict()}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load data: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to load data\")\n",
    "    \n",
    "@app.post(\"/analyze_data/\")\n",
    "async def analyze_data(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        df = pd.read_csv(file.file)\n",
    "        analysis = {\n",
    "            \"head\": df.head().to_dict(),\n",
    "            \"describe\": df.describe().to_dict(),\n",
    "            \"correlation\": df.corr().to_dict()\n",
    "        }\n",
    "        return {\"message\": \"Data analysis completed\", \"analysis\": analysis}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to analyze data: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to analyze data\")\n",
    "\n",
    "\n",
    "@app.post(\"/visualize_data/\")\n",
    "async def visualize_data(file: UploadFile = File(...)):\n",
    "    try:\n",
    "        df = pd.read_csv(file.file)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        df.hist()\n",
    "        plt_path = \"static/histogram.png\"\n",
    "        plt.savefig(plt_path)\n",
    "        plt.close()\n",
    "        return {\"message\": \"Data visualization created\", \"url\": f\"/scripts/{plt_path}\"}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to visualize data: {str(e)}\")\n",
    "        raise HTTPException(status_code=500, detail=\"Failed to visualize data\")\n",
    "\n",
    "@app.get(\"/health_check/\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"running\", \"message\": \"API is healthy\"}\n",
    "\n",
    "\n",
    "@app.post(\"/dynamic_predict/\")\n",
    "async def dynamic_predict(request: Request):\n",
    "    data = await request.json()\n",
    "    text = data.get(\"text\")\n",
    "    if not text:\n",
    "        raise HTTPException(status_code=400, detail=\"Text parameter is required\")\n",
    "\n",
    "    tokenizer, model = load_model()\n",
    "    encoded_input = tokenizer.encode_plus(\n",
    "        text, add_special_tokens=True, max_length=128,\n",
    "        return_attention_mask=True, return_tensors='pt', truncation=True\n",
    "    )\n",
    "    input_ids = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = logits.argmax().item()\n",
    "\n",
    "    description = {\n",
    "        0: \"Negative\",\n",
    "        1: \"Neutral\",\n",
    "        2: \"Positive\"\n",
    "    }.get(predicted_class_id, \"Unknown\")\n",
    "\n",
    "    return {\"text\": text, \"class_id\": predicted_class_id, \"description\": description}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def build_and_run():\n",
    "    # Build the Docker image\n",
    "    subprocess.run([\"docker\", \"build\", \"-t\", \"fastapi-model-server\", \".\"], check=True)\n",
    "    \n",
    "    # Run the Docker container\n",
    "    subprocess.run([\"docker\", \"run\", \"--rm\", \"-p\", \"8000:8000\", \"fastapi-model-server\"], check=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    build_and_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_github_actions_workflow():\n",
    "    # Define the directory path and filename for the GitHub Actions workflow\n",
    "    directory = '.github/workflows'\n",
    "    filename = 'zenml_pipeline.yml'\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Define the content of the CI/CD pipeline\n",
    "    workflow_content = \"\"\"\n",
    "    name: ZenML Pipeline Execution\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "  pull_request:\n",
    "    branches:\n",
    "      - main\n",
    "\n",
    "jobs:\n",
    "  run-zenml-pipeline:\n",
    "    runs-on: ubuntu-latest\n",
    "\n",
    "    services:\n",
    "      cassandra:\n",
    "        image: cassandra:latest\n",
    "        ports:\n",
    "          - 9042:9042\n",
    "        options: --health-cmd \"cqlsh -e 'DESCRIBE KEYSPACES'\" --health-interval 10s --health-timeout 5s --health-retries 5\n",
    "\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: \"3.9\"\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "\n",
    "      - name: Run ZenML Pipeline\n",
    "        run: |\n",
    "          python -m zenml up\n",
    "          python final_notebook.ipynb\n",
    "        env:\n",
    "          CASSANDRA_CLUSTER: \"localhost\"\n",
    "\n",
    "      - name: Shutdown ZenML services\n",
    "        if: always()\n",
    "        run: python -m zenml down\n",
    "\n",
    "      - name: Log Metrics to MLflow\n",
    "        run: |\n",
    "          pip install mlflow\n",
    "          mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host\n",
    "        env:\n",
    "          MLFLOW_TRACKING_URI: http://localhost:5000\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the workflow content to the file\n",
    "    with open(os.path.join(directory, filename), 'w') as file:\n",
    "        file.write(workflow_content)\n",
    "    print(f\"Workflow file '{filename}' created in '{directory}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_github_actions_workflow()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c1f741a4f83aa020b4b2a4d7353a073a4e5e4a855a3258a20da40294ddbf005"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
